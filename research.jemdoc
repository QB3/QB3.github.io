# jemdoc: menu{MENU}{research.html}, nofooter
==Quentin BERTRAND - Research

== Ph. D. Thesis

- *Q. Bertrand*, [https://tel.archives-ouvertes.fr/tel-03373531/document Hyperparameter selection for high dimensional sparse learning: application to neuroimaging], [./pdfs/slides_defense.pdf slides], [https://youtu.be/-CnX6qtbCYU recording of the defense]


== Papers

- 2024
-- *Q. Bertrand*, J. Duque, E. Calvano, G. Gidel [https://arxiv.org/abs/2312.08484 Q-learners Can Provably Collude in the Iterated Prisoner's Dilemma]
-- *Q. Bertrand*, A. J. Bose, A. Duplessis, M. Jiralerspong, G. Gidel [https://arxiv.org/pdf/2310.00429.pdf, On the Stability of Iterative Retraining of Generative Models on their own Data], ICLR 2024 (with spotlight!), [https://github.com/QB3/gen_models_dont_go_mad code], [./pdfs/iterative_retraining.pdf slides]

- 2023
-- J. Ramirez, R. Sukumaran, *Q. Bertrand*, G. Gidel,
[https://arxiv.org/pdf/2306.07905.pdf Omega: Optimistic EMA Gradients], ICML 2023 LatinX in AI Workshop, [https://github.com/juan43ramirez/Omega code]
-- S. Lachapelle, T. Deleu, D. Mahajan, I. Mitliagkas, Y. Bengio, S. Lacoste-Julien, *Q. Bertrand*,
[https://arxiv.org/abs/2211.14666  Synergies between Disentanglement and Sparsity:
Generalization and Identifiability in Multi-Task Learning], ICML 2023
-- *Q. Bertrand*, W. M. Czarnecki, G. Gidel [https://arxiv.org/pdf/2206.12301.pdf, On the Limitations of Elo: Real-World Games are Transitive, not Additive], AISTATS 2023
-- Q. Klopfenstein\*, *Q. Bertrand*\*, A. Gramfort, J. Salmon, S. Vaiter, [https://arxiv.org/abs/2010.11825 Model identification and local linear convergence of coordinate descent], Optimization Letters


- 2022
-- D. Scieur, *Q. Bertrand*, G. Gidel, F. Pedregosa,
[https://arxiv.org/abs/2209.13271  The Curse of Unrolling: Rate of Differentiating Through Optimization],
NeurIPS 2022
-- *Q. Bertrand*, Q. Klopfenstein\, P.-A. Bannier, G. Gidel, M. Massias [https://arxiv.org/pdf/2204.07826.pdf Beyond L1: Faster and Better Sparse Models with skglm],
NeurIPS 2022
[https://github.com/scikit-learn-contrib/skglm code],
[https://contrib.scikit-learn.org/skglm/ doc]
--  *Q. Bertrand*\*, Q. Klopfenstein\*, M. Massias, M. Blondel, S. Vaiter, A. Gramfort, J. Salmon,
        [https://arxiv.org/abs/2105.01637
        Implicit differentiation for fast hyperparameter selection in non-smooth convex learning],
        JMLR,
        [https://github.com/QB3/sparse-ho code],
        [https://qb3.github.io/sparse-ho/ doc]


- 2021
--    P.-A. Bannier, *Q. Bertrand*, J. Salmon, A. Gramfort,
        [https://hal.archives-ouvertes.fr/hal-03418092/document
        Electromagnetic neural source imaging under
        sparsity constraints with SURE-based hyperparameter tuning],
        Workshop medical imaging meets NeurIPS, NeurIPS2021,
        [https://github.com/PABannier/automatic_hp_selection_for_meg code]
--    *Q. Bertrand*, M. Massias,
        [https://arxiv.org/pdf/2011.10065.pdf Anderson acceleration of coordinate descent],
        AISTATS 2021,
        [https://github.com/mathurinm/andersoncd code],
        [https://mathurinm.github.io/andersoncd/ doc]


- 2020
--    *Q. Bertrand*\*, Q. Klopfenstein\*, M. Blondel, S. Vaiter, A. Gramfort, J. Salmon,
        [https://arxiv.org/pdf/2002.08943.pdf
        Implicit differentiation of Lasso-type models for hyperparameter optimization],
        ICML 2020,
        [https://proceedings.icml.cc/paper/2020/file/e0ab531ec312161511493b002f9be2ee-Paper.pdf proc.],
        [https://github.com/QB3/sparse-ho code],
        [https://qb3.github.io/sparse-ho/ doc]
--   M. Massias\*, *Q. Bertrand*\*, A. Gramfort, J. Salmon,
        [https://arxiv.org/abs/2001.05401 Support recovery and sup-norm convergence rates for sparse pivotal estimation],
        AISTATS 2020,
        [http://proceedings.mlr.press/v108/massias20a/massias20a.pdf proc.]

- 2019
--   *Q. Bertrand*\*, M. Massias\*, A. Gramfort, J. Salmon,
        [https://arxiv.org/abs/1902.02509 Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso], NeurIPS 2019,
        [https://papers.nips.cc/paper/8651-handling-correlated-and-repeated-measurements-with-the-smoothed-multivariate-square-root-lasso.pdf proc.],
        [https://github.com/QBE/clar code],
        [https://qb3.github.io/CLaR/ doc]


== Slides

-    [./pdfs/iterative_retraining.pdf
        On the Stability of Iterative Retraining of Generative Models on their own Data]
-    [./pdfs/slides_defense.pdf
        Hyperparameter selection for high dimensional sparse learning: application to neuroimaging],
        28\/09\/2021, Ph. D. Defense, Paris-Saclay, France.
-    [./pdfs/talks/cd.pdf
        Anderson acceleration of coordinate descent],
        07\/06\/2021, Journées des statistiques, Nice, France.
-    [./pdfs/talks/DS3_slides.pdf
        Optimization for machine learning, "Hands on"],
        04\/01\/2021, Data Science Summer School of École polytechnique, France.
-    [./pdfs/talks/sparse_ho_long.pdf
        Implicit differentiation of Lasso-type models for hyperparameter optimization],
        09\/09\/2020, SMAI MODE 2020, France.
-    [./pdfs/talks/GDRMOA2019.pdf
        Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso],
        18\/10\/2019, GDR MOA 2019, France.


== Reviewing service
- ICML 2021-2024
- ICLR 2021-2024
- NeurIPS 2020-2024 (top reviewer in 2021 and 2022)
- JMLR 2021-2024
- Neuroimage 2019-2021
- AISTATS 2021
- Electronic Journal of Statistics 2020
- IEEE SPL 2020
