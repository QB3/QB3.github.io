# jemdoc: menu{MENU}{optim_crash_course.html}, nofooter

== Mila Optimization Crash Course

== Goal

The goal of this crash course is to present standard proof techniques for simple optimization algorithms.
\n
It is tailored for practitioners who use optimization as black box tools, e.g., SGD or ADAM to optimize neural networks and want to understand the underlying principles of optimization.
\n
The overall idea is to present each building block of the ADAM optimizer.

For each lecture, proofs will be done onboard and at least 30 minutes will be dedicated to redo the proofs by yourself. Lecture notes will be released on the fly.

== When & Where

Wednesday 15h-17h, room H04 (Mila 6650 building).

== Organizers

Lucas Maes, Danilo Vucetic, Damien Scieur and Quentin Bertrand

== Attempted Schedule

The first sessions, from basic concepts to stochastic gradient descent, will be lecture-based, and the last ones will be seminar-based sessions.
\n
\n
- 02-14-2024 Basic concepts, convexity, smoothness, convergence proof of gradient descent (Lecturer: Quentin Bertrand)
-- References: Lecture notes from [https://courses.cs.washington.edu/courses/cse546/15au/lectures/lecture09_optimization.pdf Sham Kakade], [https://www.stat.cmu.edu/~ryantibs/convexopt-F13/scribes/lec6.pdf Ryan Tibshirani], [https://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-2-notes.pdf Ioannis Mitliagkas], [https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf Robert Gower], [https://mathurinm.github.io/assets/2022_ens/class.pdf Mathurin Massias] (Chapters 2 and 3). You can also check [https://pages.cs.wisc.edu/~yliang/cs839_spring22/material/Introductory-Lectures-on-Convex-Programming-Yurii-Nesterov-2004.pdf Yuri Nesterov]'s book (Chapters 1 and 2)

- 02-21-2024 Subgradient Descent and Dual averaging (Lecturer: Damien Scieur)
-- References: Lectures from [https://people.eecs.berkeley.edu/~jiantao/227c2022spring/scribe/227C_Lecture_04.pdf Jiantao Jiao]

- 02-28-2024 Acceleration: Nesterov and heavy ball (Lecturer: Damien Scieur)
-- References: Chapter 4 of [https://www.nowpublishers.com/article/DownloadSummary/OPT-036 Damien's monograph]

- 03-06-2024 Adaptive methods: line search and Polyak step size (Lecturer: Damien Scieur)

- 03-13-2024 Stochastic gradient descent (Lecturer: Quentin Bertrand)
-- References: Simon Lacoste-Julien Lecture notes on [https://www-labs.iro.umontreal.ca/~slacoste/teaching/ift6132/W24/notes/lecture10_scribbles.pdf stochastic subgradient method] and [https://www-labs.iro.umontreal.ca/~slacoste/teaching/ift6132/W24/notes/lecture11_scribbles.pdf convex optimization]

- 03-20-2024 RMSProp and ADAM (Lecturer: Charles Guille-Escuret)

- 03-27-2024 New adaptive techniques for deep learning (Lecturer: Damien Scieur)

- 04-03-2024 Edge of stability (Lecturer: Gauthier Gidel)
