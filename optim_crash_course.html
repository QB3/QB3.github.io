  <!doctype html>
  <html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\(','$\)']],
      displayMath: [['$$','$$'], ['$\[','$\]']]
    },
    TeX: {
      extensions: ["AMScd.js", "action.js", "autobold.js", "cancel.js", "begingroup.js", "color.js", "enclose.js"]
    }
  });
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="jemdoc.css" type="text/css" />
  <title></title>
  </head>
  <body>
  <div class="title-divider"></div>
  <main class="container" id="tlayout">
  <div class="row">
  <!-- Side menu -->
  <aside class="col-12 col-md-3 " id="sidemenu">
  <ul class="nav nav-pills flex-column">
  <div class="menu-category">Quentin BERTRAND</div>
  <li class="nav-item menu-item"><a href="index.html" class="nav-link">Home</a></li>
  <li class="nav-item menu-item"><a href="research.html" class="nav-link">Research</a></li>
  <li class="nav-item menu-item"><a href="teaching.html" class="nav-link">Teaching</a></li>
  <li class="nav-item menu-item"><a href="https://github.com/qb3" class="nav-link">Github</a></li>
  <li class="nav-item menu-item"><a href="optim_crash_course.html" class="nav-link active">Optim&nbsp;Crash&nbsp;Course</a></li>
  </ul>
  </aside>
  <div class="col-12 col-md-9" id="main-content">
<h2>Mila Optimization Crash Course</h2>
<h2>Goal</h2>
<p>The goal of this crash course is to present standard proof techniques for simple optimization algorithms.
<br />
It is tailored for practitioners who use optimization as black box tools, e.g., SGD or ADAM to optimize neural networks and want to understand the underlying principles of optimization.
<br />
The overall idea is to present each building block of the ADAM optimizer.</p>
<p>For each lecture, proofs will be done onboard and at least 30 minutes will be dedicated to redo the proofs by yourself. Lecture notes will be released on the fly.</p>
<h2>When &amp; Where</h2>
<p>Wednesday 15h-17h, room H04 (Mila 6650 building).</p>
<h2>Organizers</h2>
<p>Danilo Vucetic, Lucas Maes, Damien Scieur and Quentin Bertrand</p>
<h2>Attempted Schedule</h2>
<p>The first sessions, from basic concepts to stochastic gradient descent, will be lecture-based, and the last ones will be seminar-based sessions.
<br />
<br /></p>
<ul>
<li><p>02-14-2024 Basic concepts, convexity, smoothness, convergence proof of gradient descent (Lecturer: Quentin Bertrand)</p>
<ul>
<li><p>References: Lecture notes from <a href="https://courses.cs.washington.edu/courses/cse546/15au/lectures/lecture09_optimization.pdf"  >Sham Kakade</a>, <a href="https://www.stat.cmu.edu/~ryantibs/convexopt-F13/scribes/lec6.pdf"  >Ryan Tibshirani</a>, <a href="https://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-2-notes.pdf"  >Ioannis Mitliagkas</a>, <a href="https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf"  >Robert Gower</a>, <a href="https://mathurinm.github.io/assets/2022_ens/class.pdf"  >Mathurin Massias</a> (Chapters 2 and 3). You can also check <a href="https://pages.cs.wisc.edu/~yliang/cs839_spring22/material/Introductory-Lectures-on-Convex-Programming-Yurii-Nesterov-2004.pdf"  >Yuri Nesterov</a>'s book (Chapters 1 and 2)</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>02-21-2024 Subgradient Descent and Dual averaging (Lecturer: Damien Scieur)</p>
<ul>
<li><p>References: Lectures notes from <a href="https://people.eecs.berkeley.edu/~jiantao/227c2022spring/scribe/227C_Lecture_04.pdf"  >Jiantao Jiao</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>02-28-2024 Convergence proof of gradient and subgradient descent Part 2 (Lecturer: Quentin Bertrand)</p>
</li>
</ul>
<ul>
<li><p>03-06-2024 Acceleration: Nesterov and heavy ball (Lecturer: Damien Scieur)</p>
<ul>
<li><p>References: Chapter 4 of <a href="https://www.nowpublishers.com/article/DownloadSummary/OPT-036"  >Damien's monograph</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>03-13-2024 Stochastic gradient descent (Lecturer: Quentin Bertrand)</p>
<ul>
<li><p>References: Fabian Pedregosa blog posts for <a href="https://fa.bianp.net/teaching/2018/COMP-652/stochastic_gradient.html"  >intuitions</a> and <a href="https://fa.bianp.net/blog/2021/exponential-sgd/"  >proofs</a>, <a href="http://www.seas.ucla.edu/~vandenbe/236C/lectures/gradient.pdf"  >Lieven Vandenberghe Lecture's notes</a>, Simon Lacoste-Julien Lecture notes on <a href="https://www-labs.iro.umontreal.ca/~slacoste/teaching/ift6132/W24/notes/lecture10_scribbles.pdf"  >stochastic subgradient method</a> and <a href="https://www-labs.iro.umontreal.ca/~slacoste/teaching/ift6132/W24/notes/lecture11_scribbles.pdf"  >convex optimization</a>, <a href="https://arxiv.org/pdf/2301.11235.pdf"  >Guillaume Garrigos and Robert Gower monography</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>03-20-2024 Adaptive methods: line search and Polyak step size (Lecturer: Damien Scieur)</p>
</li>
</ul>
<ul>
<li><p>03-27-2024 RMSProp and ADAM (Lecturer: Charles Guille-Escuret)</p>
</li>
</ul>
<ul>
<li><p>04-17-2024 Automated proof techniques (Lecturer: Gauthier Gidel)</p>
</li>
</ul>
<ul>
<li><p>04-24-2024 New adaptive techniques for deep learning (Lecturer: Damien Scieur)</p>
</li>
</ul>
  <!-- End of main body -->
  </div>
  </div>
  </main>
  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
  </body>
  </html>
