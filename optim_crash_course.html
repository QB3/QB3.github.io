<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Quentin BERTRAND</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="https://github.com/qb3">Github</a></div>
<div class="menu-item"><a href="optim_crash_course.html" class="current">Optim&nbsp;Crash&nbsp;Course</a></div>
</td>
<td id="layout-content">
<h2>Mila Optimization Crash Course</h2>
<h2>Goal</h2>
<p>The goal of this crash course is to present standard proof techniques for simple optimization algorithms.
<br />
It is tailored for practitioners who use optimization as black box tools, e.g., SGD or ADAM to optimize neural networks and want to understand the underlying principles of optimization.
<br />
The overall idea is to present each building block of the ADAM optimizer.</p>
<p>For each lecture, proofs will be done onboard and at least 30 minutes will be dedicated to redo the proofs by yourself. Lecture notes will be released on the fly.</p>
<h2>When &amp; Where</h2>
<p>Wednesday 15h-17h, room H04 (Mila 6650 building).</p>
<h2>Organizers</h2>
<p>Lucas Maes, Danilo Vucetic, Damien Scieur and Quentin Bertrand</p>
<h2>Attempted Schedule</h2>
<p>The first sessions, from basic concepts to stochastic gradient descent, will be lecture-based, and the last ones will be seminar-based sessions.
<br />
<br /></p>
<ul>
<li><p>02-14-2024 Basic concepts, convexity, smoothness, convergence proof of gradient descent (Lecturer: Quentin Bertrand)</p>
<ul>
<li><p>References: Lecture notes from <a href="https://courses.cs.washington.edu/courses/cse546/15au/lectures/lecture09_optimization.pdf">Sham Kakade</a>, <a href="https://www.stat.cmu.edu/~ryantibs/convexopt-F13/scribes/lec6.pdf">Ryan Tibshirani</a>, <a href="https://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-2-notes.pdf">Ioannis Mitliagkas</a>, <a href="https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf">Robert Gower</a>, <a href="https://mathurinm.github.io/assets/2022_ens/class.pdf">Mathurin Massias</a> (Chapters 2 and 3). You can also check <a href="https://pages.cs.wisc.edu/~yliang/cs839_spring22/material/Introductory-Lectures-on-Convex-Programming-Yurii-Nesterov-2004.pdf">Yuri Nesterov</a>'s book (Chapters 1 and 2)</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>02-21-2024 Dual averaging (Lecturer: Damien Scieur)</p>
</li>
</ul>
<ul>
<li><p>02-28-2024 Acceleration: Nesterov and heavy ball (Lecturer: Damien Scieur)</p>
<ul>
<li><p>References: Chapter 4 of <a href="https://www.nowpublishers.com/article/DownloadSummary/OPT-036">Damien's monograph</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>03-06-2024 Adaptive methods: line search and Polyak step size (Lecturer: Damien Scieur)</p>
</li>
</ul>
<ul>
<li><p>03-13-2024 Stochastic gradient descent (Lecturer: Quentin Bertrand)</p>
</li>
</ul>
<ul>
<li><p>03-20-2024 RMSProp and ADAM (Lecturer: Charles Guille-Escuret)</p>
</li>
</ul>
<ul>
<li><p>03-27-2024 New adaptive techniques for deep learning (Lecturer: Damien Scieur)</p>
</li>
</ul>
<ul>
<li><p>04-03-2024 Edge of stability (Lecturer: Gauthier Gidel)</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
