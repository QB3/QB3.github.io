# jemdoc: menu{MENU}{index.html}, nofooter
==Quentin BERTRAND - Home

~~~
{}{img_left}{./london_cropped.png}{Picture of me}{207}{194}{}

Since July 1st, I am a researcher (`charg√© de recherche`) at Inria Lyon, located in Laboratoire Hubert Curien.
I work on optimization, games, and representation learning.

From November 2021 to June 2024, I was a post-doctoral researcher at [https://mila.quebec/ Mila] working with [https://gauthiergidel.github.io/ Gauthier Gidel] and [https://www.iro.umontreal.ca/~slacoste/ Simon Lacoste-Julien].
\n
Prior to this position, I did my Ph. D. at Inria Paris-Saclay
(in the [https://team.inria.fr/parietal/ Parietal Team]) under the supervision of [https://josephsalmon.eu/ Joseph Salmon] and [http://alexandre.gramfort.net/ Alexandre Gramfort].
\n
I worked on the optimization and statistical aspects of high dimensional sparse linear regression applied to brain signal reconstruction.
\n
In particular, I developed Python packages for fast [https://github.com/scikit-learn-contrib/skglm computation] and [https://github.com/QB3/sparse-ho automatic hyperparameter selection] of sparse linear models.

\n
Here is a short [./pdfs/Quentin_BERTRAND_CV.pdf resume] and my [research.html list of publications].


~~~

== Contact
~~~
Email: quentin \[dot\] bertrand AT inria \[dot\] fr\n
~~~


== News

- 04-22-2024 The recording of the talk [https://arxiv.org/pdf/2310.00429.pdf, On the Stability of Iterative Retraining of Generative Models on their own Data] at the Montreal Machine Learning Seminar can be found [https://www.youtube.com/watch?v=ZOLHzmSZWrA here]
- 01-16-2024 Our paper [https://arxiv.org/pdf/2310.00429.pdf, On the Stability of Iterative Retraining of Generative Models on their own Data] was accepted to ICLR 2024 with a spotlight, see you in Vienna!
- 12-18-2023 We just released our paper proving that [https://arxiv.org/abs/2312.08484 Q-learners can provably learn to collude in the iterated prisoner dilemma]!
- 01-07-2023 On July 1st, 2024 I will join Inria as a Research Scientist!

== Previous News
- 05-12-2023 I will present our paper [https://arxiv.org/pdf/2206.12301.pdf, On the Limitations of Elo: Real-World Games are Transitive, not Additive] at the [https://sites.google.com/view/berkeleymarl/home Berkeley Multi-Agent Reinforcement Learning Seminar]
- Our paper [https://arxiv.org/abs/2211.14666  Synergies between Disentanglement and Sparsity:
Generalization and Identifiability in Multi-Task Learning
] has been accepted at ICML 2023, see you in Hawaii!
- Our paper [https://arxiv.org/pdf/2206.12301.pdf, On the Limitations of Elo: Real-World Games are Transitive, not Additive] has been accepted to AISTATS 2023, see you in Spain!
- I just presented our paper [https://arxiv.org/abs/2211.14666  Synergies between Disentanglement and Sparsity: a Multi-task Learning Perspective] at the [https://winter22.cms.math.ca/ Canadian Mathematical Society Winter Workshop]
- I just presented our two papers [https://arxiv.org/pdf/2204.07826.pdf Beyond L1: Faster and Better Sparse Models with skglm] and [https://arxiv.org/abs/2209.13271 The Curse of Unrolling: Rate of Differentiating Through Optimization] at NeurIPS 2022
- I was awarded the [https://neurips.cc/Conferences/2022/ProgramCommittee top reviewer award] at NeurIPS 2022!
- Our papers [https://arxiv.org/pdf/2204.07826.pdf Beyond L1: Faster and Better Sparse Models with skglm] and [https://arxiv.org/abs/2209.13271 The Curse of Unrolling: Rate of Differentiating Through Optimization] have been accepted to NeurIPS 2022!
