# jemdoc: menu{MENU}{index.html}, nofooter
==Quentin BERTRAND - Home

~~~
{}{img_left}{./neurips2024.JPG}{Picture of me}{207}{194}{}

Since July 1st 2024, I am a junior tenured researcher (`chargé de recherche`) at [https://www.inria.fr/en/inria-lyon-centre Inria Lyon] and [https://www.univ-st-etienne.fr/en/index.html Université Jean Monnet], in the [https://labhc-malice.github.io/ Malice team], located in [https://laboratoirehubertcurien.univ-st-etienne.fr/en/index.html Laboratoire Hubert Curien]. I teach at [https://www.ens-lyon.fr/ ENS Lyon].
I currently work on optimization and generative models.

From November 2021 to June 2024, I was a post-doctoral researcher at [https://mila.quebec/ Mila] working with [https://gauthiergidel.github.io/ Gauthier Gidel] and [https://www.iro.umontreal.ca/~slacoste/ Simon Lacoste-Julien].
Prior to this position, I did my Ph. D. at Inria Paris-Saclay
(in the [https://team.inria.fr/parietal/ Parietal Team]) under the supervision of [https://josephsalmon.eu/ Joseph Salmon] and [http://alexandre.gramfort.net/ Alexandre Gramfort].
I worked on the optimization and statistical aspects of high dimensional brain signal reconstruction.

In particular,
- We just released a [https://dl.heeere.com/conditional-flow-matching/blog/conditional-flow-matching/ friendly blog post] on normalizing flows and conditional flow matching techniques!
- Our recent works on [https://arxiv.org/pdf/2310.00429.pdf self-consuming generative models] and [https://arxiv.org/abs/2407.09499 their biases] got [https://www.nytimes.com/interactive/2024/08/26/upshot/ai-synthetic-data.html media coverage from the N.Y. times] and the [https://www.theglobeandmail.com/business/article-training-ai-models-generated-data-llms/ Canada Globe and Mail]!
- We released a [https://github.com/scikit-learn-contrib/skglm Python package for large scale optimization] of sparse problems ([https://pepy.tech/project/skglm 4k download/month]).


\n
Here is a short [./pdfs/Quentin_BERTRAND_CV.pdf resume] and my [research.html list of publications].


~~~

== Contact
~~~
Email: quentin \[dot\] bertrand AT inria \[dot\] fr\n
~~~


== News
- We organize a [https://smai2025.math.cnrs.fr/fr/programme/minisymposia/ minisymphosium on Generative Models and Optimal Transport] for the [https://smai2025.math.cnrs.fr/fr/ 2025 Biennal SMAI Conference].
It will take place from June 2nd to June 6th, see you in Carcans-Maubuissons!
- 05-02-2025 Our paper [https://arxiv.org/abs/2312.08484 Q-learners Can Provably Collude in the Iterated Prisoner's Dilemma] was just accepted to ICML, see you in Vancouver!
- 04-08-2025 We just gave a 12h tutorial on deep generative models at the [https://ascii.org.sn/ Senegalese Computer Science Society] and [https://ai-hubsenegal.sn/en/ AI Hub Sénégal] Summer School. The material can be found [https://github.com/QB3/SenHubIA2025 here]. Thanks to [https://www.inria.fr/en Inria] and the [https://sn.ambafrance.org/ French Embassy] for making this possible: we had amazing interactions in Dakar!
- 03-20-2025 Our Inria-Mila associated team was just created. See you soon in Montréal!
- 02-01-2025 Our [https://dl.heeere.com/conditional-flow-matching/blog/conditional-flow-matching/ friendly blog post] on normalizing flows and conditional flow matching techniques was accepted at the ICLR 2025 Blog Post Track.
- 01-15-2025 I just gave a talk on how to retrain on synthetic data at the [https://rt-maiages.math.cnrs.fr/mia25/program/ Mathematic Image and Applications conference]! Here are the [./pdfs/iterative_retraining_preference.pdf slides]
- 12-09-2024 I was at NeurIPS to present our paper [https://arxiv.org/abs/2407.09499 Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences]
- 11-26-2024 We just released a [https://dl.heeere.com/conditional-flow-matching/blog/conditional-flow-matching/  blog post] on normalizing flows and conditional flow matching techniques!
- 09-29-2024 I was delighted to be a keynote speaker for the ECCV workshop [https://sites.google.com/view/darksideofgenaiandbeyond ¨The Dark Side of Generative AIs and Beyond¨], here are the [./pdfs/iterative_retraining_preference.pdf slides]
- 09-26-2024 Our paper showing that [https://arxiv.org/abs/2407.09499 Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences] was just accepted to NeurIPS! See you in Vancouver!
- 08-26-2024 Our recent works on [https://arxiv.org/pdf/2310.00429.pdf self-consuming generative models] and [https://arxiv.org/abs/2407.09499 their biases] was [https://www.nytimes.com/interactive/2024/08/26/upshot/ai-synthetic-data.html featured in the N.Y. times]!
- 07-01-2024 Just started as an Inria researcher in the [https://labhc-malice.github.io/ Malice team]
- 04-22-2024 The recording of the talk [https://arxiv.org/pdf/2310.00429.pdf, On the Stability of Iterative Retraining of Generative Models on their own Data] at the Montreal Machine Learning Seminar can be found [https://www.youtube.com/watch?v=ZOLHzmSZWrA here]
- 01-16-2024 Our paper [https://arxiv.org/pdf/2310.00429.pdf, On the Stability of Iterative Retraining of Generative Models on their own Data] was accepted to ICLR 2024 with a spotlight, see you in Vienna!
- 12-18-2023 We just released our paper proving that [https://arxiv.org/abs/2312.08484 Q-learners can provably learn to collude in the iterated prisoner dilemma]!
- 01-07-2023 On July 1st, 2024 I will join Inria as a Research Scientist!

== Previous News
- 05-12-2023 I will present our paper [https://arxiv.org/pdf/2206.12301.pdf, On the Limitations of Elo: Real-World Games are Transitive, not Additive] at the [https://sites.google.com/view/berkeleymarl/home Berkeley Multi-Agent Reinforcement Learning Seminar]
- Our paper [https://arxiv.org/abs/2211.14666  Synergies between Disentanglement and Sparsity:
Generalization and Identifiability in Multi-Task Learning
] has been accepted at ICML 2023, see you in Hawaii!
- Our paper [https://arxiv.org/pdf/2206.12301.pdf, On the Limitations of Elo: Real-World Games are Transitive, not Additive] has been accepted to AISTATS 2023, see you in Spain!
- I just presented our paper [https://arxiv.org/abs/2211.14666  Synergies between Disentanglement and Sparsity: a Multi-task Learning Perspective] at the [https://winter22.cms.math.ca/ Canadian Mathematical Society Winter Workshop]
- I just presented our two papers [https://arxiv.org/pdf/2204.07826.pdf Beyond L1: Faster and Better Sparse Models with skglm] and [https://arxiv.org/abs/2209.13271 The Curse of Unrolling: Rate of Differentiating Through Optimization] at NeurIPS 2022
- I was awarded the [https://neurips.cc/Conferences/2022/ProgramCommittee top reviewer award] at NeurIPS 2022!
- Our papers [https://arxiv.org/pdf/2204.07826.pdf Beyond L1: Faster and Better Sparse Models with skglm] and [https://arxiv.org/abs/2209.13271 The Curse of Unrolling: Rate of Differentiating Through Optimization] have been accepted to NeurIPS 2022!
