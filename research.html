<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Quentin BERTRAND - Research</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Quentin BERTRAND</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="https://github.com/qb3">Github</a></div>
<div class="menu-item"><a href="https://www.linkedin.com/in/quentin--bertrand/">LinkedIn</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Quentin BERTRAND - Research</h1>
</div>
<h2>Ph. D. Thesis</h2>
<ul>
<li><p><b>Q. Bertrand</b>, <a href="https://tel.archives-ouvertes.fr/tel-03373531/document">Hyperparameter selection for high dimensional sparse learning: application to neuroimaging</a>, <a href="./pdfs/slides_defense.pdf">slides</a>, <a href="https://youtu.be/-CnX6qtbCYU">recording of the defense</a></p>
</li>
</ul>
<h2>Papers</h2>
<ul>
<li><p>2023</p>
<ul>
<li><p>S. Lachapelle, T. Deleu, D. Mahajan, I. Mitliagkas, Y. Bengio, S. Lacoste-Julien, <b>Q. Bertrand</b>,
<a href="https://arxiv.org/abs/2211.14666">Synergies between Disentanglement and Sparsity: a Multi-task Learning Perspective</a> (Submitted)</p>
</li>
<li><p><b>Q. Bertrand</b>, W. M. Czarnecki, G. Gidel <a href="https://arxiv.org/pdf/2206.12301.pdf,">On the Limitations of Elo: Real-World Games are Transitive, not Additive</a> (Submitted)</p>
</li>
<li><p>Q. Klopfenstein*, <b>Q. Bertrand</b>*, A. Gramfort, J. Salmon, S. Vaiter, <a href="https://arxiv.org/abs/2010.11825">Model identification and local linear convergence of coordinate descent</a>, Optimization Letters</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>2022</p>
<ul>
<li><p>D. Scieur, <b>Q. Bertrand</b>, G. Gidel, F. Pedregosa,
<a href="https://arxiv.org/abs/2209.13271">The Curse of Unrolling: Rate of Differentiating Through Optimization</a>,
NeurIPS 2022</p>
</li>
<li><p><b>Q. Bertrand</b>, Q. Klopfenstein, P.-A. Bannier, G. Gidel, M. Massias <a href="https://arxiv.org/pdf/2204.07826.pdf">Beyond L1: Faster and Better Sparse Models with skglm</a>,
NeurIPS 2022
<a href="https://github.com/scikit-learn-contrib/skglm">code</a>,
<a href="https://contrib.scikit-learn.org/skglm/">doc</a></p>
</li>
<li><p><b>Q. Bertrand</b>*, Q. Klopfenstein*, M. Massias, M. Blondel, S. Vaiter, A. Gramfort, J. Salmon,
<a href="https://arxiv.org/abs/2105.01637">Implicit differentiation for fast hyperparameter selection in non-smooth convex learning</a>,
JMLR,
<a href="https://github.com/QB3/sparse-ho">code</a>,
<a href="https://qb3.github.io/sparse-ho/">doc</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>2021</p>
<ul>
<li><p>P.-A. Bannier, <b>Q. Bertrand</b>, J. Salmon, A. Gramfort,
<a href="https://hal.archives-ouvertes.fr/hal-03418092/document">Electromagnetic neural source imaging under
sparsity constraints with SURE-based hyperparameter tuning</a>,
Workshop medical imaging meets NeurIPS, NeurIPS2021,
<a href="https://github.com/PABannier/automatic_hp_selection_for_meg">code</a></p>
</li>
<li><p><b>Q. Bertrand</b>, M. Massias,
<a href="https://arxiv.org/pdf/2011.10065.pdf">Anderson acceleration of coordinate descent</a>,
AISTATS 2021,
<a href="https://github.com/mathurinm/andersoncd">code</a>,
<a href="https://mathurinm.github.io/andersoncd/">doc</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>2020</p>
<ul>
<li><p><b>Q. Bertrand</b>*, Q. Klopfenstein*, M. Blondel, S. Vaiter, A. Gramfort, J. Salmon,
<a href="https://arxiv.org/pdf/2002.08943.pdf">Implicit differentiation of Lasso-type models for hyperparameter optimization</a>,
ICML 2020,
<a href="https://proceedings.icml.cc/paper/2020/file/e0ab531ec312161511493b002f9be2ee-Paper.pdf">proc.</a>,
<a href="https://github.com/QB3/sparse-ho">code</a>,
<a href="https://qb3.github.io/sparse-ho/">doc</a></p>
</li>
<li><p>M. Massias*, <b>Q. Bertrand</b>*, A. Gramfort, J. Salmon,
<a href="https://arxiv.org/abs/2001.05401">Support recovery and sup-norm convergence rates for sparse pivotal estimation</a>,
AISTATS 2020,
<a href="http://proceedings.mlr.press/v108/massias20a/massias20a.pdf">proc.</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>2019</p>
<ul>
<li><p><b>Q. Bertrand</b>*, M. Massias*, A. Gramfort, J. Salmon,
<a href="https://arxiv.org/abs/1902.02509">Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso</a>, NeurIPS 2019,
<a href="https://papers.nips.cc/paper/8651-handling-correlated-and-repeated-measurements-with-the-smoothed-multivariate-square-root-lasso.pdf">proc.</a>,
<a href="https://github.com/QBE/clar">code</a>,
<a href="https://qb3.github.io/CLaR/">doc</a></p>
</li>
</ul>

</li>
</ul>
<h2>Slides</h2>
<ul>
<li><p><a href="./pdfs/slides_defense.pdf">Hyperparameter selection for high dimensional sparse learning: application to neuroimaging</a>,
28/09/2021, Ph. D. Defense, Paris-Saclay, France.</p>
</li>
<li><p><a href="./pdfs/talks/cd.pdf">Anderson acceleration of coordinate descent</a>,
07/06/2021, Journées des statistiques, Nice, France.</p>
</li>
<li><p><a href="./pdfs/talks/DS3_slides.pdf">Optimization for machine learning, &ldquo;Hands on&rdquo;</a>,
04/01/2021, Data Science Summer School of École polytechnique, France.</p>
</li>
<li><p><a href="./pdfs/talks/sparse_ho_long.pdf">Implicit differentiation of Lasso-type models for hyperparameter optimization</a>,
09/09/2020, SMAI MODE 2020, France.</p>
</li>
<li><p><a href="./pdfs/talks/GDRMOA2019.pdf">Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso</a>,
18/10/2019, GDR MOA 2019, France.</p>
</li>
</ul>
<h2>Reviewing service</h2>
<ul>
<li><p>ICML 2021-2022</p>
</li>
<li><p>AISTATS 2021</p>
</li>
<li><p>ICLR 2021-2022</p>
</li>
<li><p>NeurIPS 2020-2022 (top reviewer in 2021 and 2022)</p>
</li>
<li><p>JMLR 2021-2022</p>
</li>
<li><p>Neuroimage 2019-2021</p>
</li>
<li><p>Electronic Journal of Statistics 2020</p>
</li>
<li><p>IEEE SPL 2020</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
